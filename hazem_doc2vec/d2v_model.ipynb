{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Importing to be used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries for pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from hazem_doc2vec.helper_functions import in_pickle, out_pickle, shuffle_corpus_labels\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import data_analysis.preprocessor_end as pre\n",
    "import os"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Defining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to divide both negative and positive text files into 2 parts, one for\n",
    "# test and the other for train. It returns two lists for train, test.\n",
    "def divide_corpus(p_corpus, n_corpus, factor):\n",
    "    train_corpus = []\n",
    "    train_labels = []\n",
    "\n",
    "    test_corpus = []\n",
    "    test_labels = []\n",
    "\n",
    "    # For positive dataset\n",
    "    n_pos = int(math.ceil(factor * len(p_corpus)))\n",
    "    for doc_id in range(len(p_corpus)):\n",
    "        if doc_id < n_pos:\n",
    "            train_corpus.append(TaggedDocument(p_corpus[doc_id], [doc_id]))\n",
    "            train_labels.append([1])\n",
    "        else:\n",
    "            test_corpus.append(p_corpus[doc_id])\n",
    "            test_labels.append([1])\n",
    "    \n",
    "    # For negative dataset\n",
    "    n_neg = int(math.ceil(factor * len(n_corpus))) \n",
    "    for doc_id in range(len(n_corpus)):\n",
    "        if doc_id < n_neg:\n",
    "            train_corpus.append(TaggedDocument(n_corpus[doc_id], [int(n_pos + doc_id)]))\n",
    "            train_labels.append([0])\n",
    "        else:\n",
    "            test_corpus.append(n_corpus[doc_id])\n",
    "            test_labels.append([0])\n",
    "            \n",
    "    return train_corpus, train_labels, test_corpus, test_labels\n",
    "\n",
    "\n",
    "def prepare_classifier_data(model, labels_arr):\n",
    "    \n",
    "    x = np.array(model.docvecs.vectors_docs)\n",
    "    y = np.zeros(model.docvecs.count, dtype=np.int)\n",
    "    \n",
    "    for i in range(model.docvecs.count):\n",
    "        y[i] = labels_arr[i][0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def prepare_classifier_test_arrays(model, test_corpus, labels_arr):\n",
    "    test_arrays = np.zeros([len(test_corpus), model.vector_size])\n",
    "    test_labels_arrays = np.zeros(len(test_corpus), dtype=np.int)\n",
    "\n",
    "    # Shuffle test data\n",
    "    test_corpus, labels_arr = shuffle_corpus_labels(test_corpus, labels_arr)\n",
    "\n",
    "    for i in range(len(test_corpus)):\n",
    "        test_arrays[i] = model.infer_vector(test_corpus[i])\n",
    "        test_labels_arrays[i] = labels_arr[i][0]\n",
    "    return test_arrays, test_labels_arrays"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Pickle in the negative and positive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.8 s, sys: 795 ms, total: 4.59 s\nWall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_corpus = in_pickle('data/pos_corpus')\n",
    "neg_corpus = in_pickle('data/neg_corpus')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.15 s, sys: 672 ms, total: 7.82 s\nWall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus, labels, temp_x, temp_y = divide_corpus(pos_corpus, neg_corpus, 1)\n",
    "out_pickle(\"data/corpus\", corpus)\n",
    "out_pickle(\"data/labels\", labels)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Pickling in the corpus and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6294 6294\nCPU times: user 4.13 s, sys: 678 ms, total: 4.8 s\nWall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = in_pickle('data/corpus')\n",
    "labels = in_pickle('data/labels')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Now, building up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_0\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_1\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_2\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_3\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_4\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nModel Saved\n\nCPU times: user 1h 13min 23s, sys: 28.6 s, total: 1h 13min 52s\nWall time: 23min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Model's parameter\n",
    "max_epochs = 5\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "# Note: defining 'dm=1' is important here. It means that we have selected \n",
    "# distributed memory’ (PV-DM) over ‘distributed bag of words’ (PV-DBOW) 'dm =0'\n",
    "# Which doesn't preserve the order of the words.\n",
    "model = Doc2Vec(min_count=1, dm=1, workers=16, window=10, vector_size=vec_size, \n",
    "                alpha=alpha, min_alpha=0.00025)\n",
    "\n",
    "# Setting up the vocabulary \n",
    "model.build_vocab(corpus)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    new_corpus, new_labels = shuffle_corpus_labels(corpus, labels)\n",
    "    corpus = new_corpus\n",
    "    labels = new_labels\n",
    "\n",
    "    print('iteration_{0}'.format(epoch), end='\\t')\n",
    "\n",
    "    model.train(corpus, total_examples=len(corpus), epochs=model.epochs)\n",
    "        \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    \n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"data/d2v.model\")\n",
    "print(\"\\nModel Saved\\n\")\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 116 ms, total: 1.44 s\nWall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading the saved doc2vec model\n",
    "model = Doc2Vec.load('data/d2v.model')\n",
    "\n",
    "# X: numpy array, holds the corpus, each doc in the corpus is represented in 300-vector\n",
    "# Y: numpy array, holds the labels of each doc in the corpus\n",
    "X, Y = prepare_classifier_data(model, labels)\n",
    "\n",
    "out_pickle('data/X', X)\n",
    "out_pickle('data/Y', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 ms, sys: 4.01 ms, total: 5.09 ms\nWall time: 4.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = in_pickle('data/X')\n",
    "Y = in_pickle('data/Y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
